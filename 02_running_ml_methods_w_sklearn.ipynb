{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c028f712-ac1c-486b-a201-16ed4557c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a53ab1-dcdf-43d5-aa97-036581e85ede",
   "metadata": {},
   "source": [
    "# 0. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b24f98-8422-41fb-82fb-0ccbc457ed61",
   "metadata": {},
   "source": [
    "Os métodos de aprendizado de máquina que vamos empregar utilizam um subconjunto dos dados disponíveis para _treinar_ um algoritmo e outros subconjunto para _testar_ o resultado do treino e verificar a pe rformance do método em dados que não foram utilizados para o treinamento.\n",
    "\n",
    "Esta linguagem é genérica e se aplica a todos os algoritmos, o que é prático para discutir os passos de uma dada análise. Por outro lado, ela obscurece o que está ocorrendo de fato no período de treinamento. Cada método possui seu próprio algoritmo, e estes possuem—na maior parte dos casos!—bases matemáticas robustas, calcadas na estatística e na teoria de otimização.\n",
    "\n",
    "Neste notebook, vamos criar conjuntos de treino e teste, aplicar cada um dos algoritmos acima aos nossos dados e avaliar sua performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c0b55-9c24-4594-a181-f755a478531b",
   "metadata": {},
   "source": [
    "# 1. Preparação de amostras de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "808417c3-de5e-4631-86cc-f198e2a9a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first CSV file (assumes it contains the test data)\n",
    "data = pd.read_csv(\"./data/des_match_vvds_clean.csv\")\n",
    "\n",
    "# The 'z' columns is the target and the 'mag_*' and 'mag_err_' columns are the predictor\n",
    "y = data['z']\n",
    "X = data[['mag_auto_g_dered',\n",
    "          'mag_auto_r_dered',\n",
    "          'mag_auto_i_dered',\n",
    "          'mag_auto_z_dered',\n",
    "          'mag_auto_y_dered',\n",
    "          'magerr_auto_g',\n",
    "          'magerr_auto_r',\n",
    "          'magerr_auto_i',\n",
    "          'magerr_auto_z',\n",
    "          'magerr_auto_y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9557e89",
   "metadata": {},
   "source": [
    "**Ex. 1: Split the data into training and test sets (e.g., 70/30 split)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132da246-95ef-42cf-a270-260a5b93b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3215c7f-4da6-4b68-9f62-01c40c4eb6e5",
   "metadata": {},
   "source": [
    "# 2. Treino/teste de três algoritmos de aprendizado de máquina e avaliação de sua performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3993f-dada-4c6e-9885-cdf32f02ea59",
   "metadata": {},
   "source": [
    "# 2.1 Treinando e testando os algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409022d5-2c91-4a45-a0ab-c3249c91b5a4",
   "metadata": {},
   "source": [
    "Nesta subseção, vamos treinar e testar uma _árvore de decisão_, uma _rede neural simples_ (\"multi-layer perceptron\") e uma _floresta aleatória_ nos dados que preparamos.  As três rodadas utilizarão a interface do scikit-learn para padronizar o código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76172d56-9a62-4e44-ad62-792faf93ccd2",
   "metadata": {},
   "source": [
    "### 2.1.1. Rede neural (Multi-layer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957b25b-98d2-468d-bc03-5be09a3d0110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron Validation MAE: 0.1521739975750021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "mlp_test_mae = mean_absolute_error(y_test, y_pred_mlp)\n",
    "print(\"Multi-Layer Perceptron Validation MAE:\", mlp_test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b79256-3c29-4c63-8cc0-1f640cbb8cd7",
   "metadata": {},
   "source": [
    "### 2.1.2. Árvore de decisão (Decision tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98abef2-7ee3-45fb-8cf4-c057bc46a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation MAE: 0.1393717011128776\n"
     ]
    }
   ],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc937d9-1bd8-45f5-97b6-1df52fc6e04e",
   "metadata": {},
   "source": [
    "### 2.1.3. Floresta aleatória (Random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0445a7a-efb2-4c8d-b244-f9ebd476d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation MAE: 0.1008815294117647\n"
     ]
    }
   ],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb446e56-8d05-4d20-875d-e0099e9ff47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare plots from three results\n",
    "\n",
    "titles = [\"Multi-Layer perceptron\", \"Decision tree\", \"Random forest\"]\n",
    "data_sets = [y_pred_mlp, y_pred_dt, y_pred_rf]\n",
    "colors = [\"C0\", \"C1\", \"C2\"]\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Random Forest\")\n",
    "plt.scatter(y_test, y_pred_rf, s=2, color=\"C0\")\n",
    "x = np.linspace(0, 1.6, 100)\n",
    "plt.plot(x, x, \"k--\", alpha=0.5)\n",
    "plt.ylim(0, 1.6)\n",
    "plt.xlim(0, 1.6)\n",
    "plt.xlabel(r\"$z_\\mathrm{true}$\")\n",
    "plt.ylabel(r\"$z_\\mathrm{phot}$\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.scatter(y_test, y_pred_dt, s=2, color=\"C1\")\n",
    "x = np.linspace(0, 1.6, 100)\n",
    "plt.plot(x, x, \"k--\", alpha=0.5)\n",
    "plt.ylim(0, 1.6)\n",
    "plt.xlim(0, 1.6)\n",
    "plt.xlabel(r\"$z_\\mathrm{true}$\")\n",
    "plt.ylabel(r\"$z_\\mathrm{phot}$\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Multi-Layer Perceptron\")\n",
    "plt.scatter(y_test, y_pred_mlp, s=2, color=\"C2\")\n",
    "x = np.linspace(0, 1.6, 100)\n",
    "plt.plot(x, x, \"k--\", alpha=0.5)\n",
    "plt.ylim(0, 1.6)\n",
    "plt.xlim(0, 1.6)\n",
    "_ = plt.xlabel(r\"$z_\\mathrm{true}$\")\n",
    "_ = plt.ylabel(r\"$z_\\mathrm{phot}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e02730-afdd-4f4a-8f8d-22b9dba22cb6",
   "metadata": {},
   "source": [
    "# 3. Quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2115958f-242e-413e-8e5d-f76cba80d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(zspec,\n",
    "                 zphot,\n",
    "                 maximum,\n",
    "                 path_to_save='',\n",
    "                 title=None,\n",
    "                 initial=0):\n",
    "    \n",
    "    '''\n",
    "    Function to plot Bias, Sigma_68, Out2σ, Out3σ given a spectroscopic and photometric redshift. \n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    zspec: Numpy array with the spectroscopic redshift.\n",
    "    \n",
    "    zphot: Numpy array with the photometric redshifts calculated. Same size as zspec.\n",
    "    \n",
    "    maximum: Float that indicates the redshift max of the plots.\n",
    "    \n",
    "    Kwargs:\n",
    "    \n",
    "    initial: Float that indicates the redshift min of the plots.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    bins = np.arange(0, 1.6, 0.1)\n",
    "    points = bins + 0.05\n",
    "    fraction_outliers = []\n",
    "    sigma68z = []\n",
    "    sigmaz = []\n",
    "    meanz = []\n",
    "    outliers_2 = []\n",
    "    drop_bins = []\n",
    "\n",
    "    for index in range(len(bins) - 1):\n",
    "        bin_lower = bins[index]\n",
    "        bin_upper = bins[index + 1]\n",
    "\n",
    "        binmask = (zphot >= bin_lower) & (zphot <= bin_upper)\n",
    "\n",
    "        if not np.any(binmask):\n",
    "            drop_bins.append(index)\n",
    "            continue\n",
    "        \n",
    "        values_r = zphot[binmask]\n",
    "        values_s = zspec[binmask]\n",
    "\n",
    "        deltabias = (values_r - values_s)\n",
    "        mean_bias = np.mean(deltabias)  # Mean bias for each bin\n",
    "        meanz.append(mean_bias)\n",
    "        \n",
    "        # FIXME: This part of the code is really bad. That's not how you calculate a quantile!\n",
    "        s = np.sort(np.abs(deltabias/(1 + values_s))) # Standard deviation (sigma) for each bin\n",
    "        sigma68 = s[int(len(s)*0.68)]\n",
    "        sigma68z.append(sigma68)\n",
    "        \n",
    "        # FIXME: This part of the code is really bad. That's not how you calculate a standard deviation!\n",
    "        sigma = (np.sum((values_r-values_s-mean_bias)**2)/len(values_r))**0.5\n",
    "        sigmaz.append(sigma)\n",
    "    \n",
    "        # Calculate the fraction of outliers outside 3 sigma\n",
    "        outliers = deltabias[np.abs(deltabias - mean_bias) > 3 * sigma]\n",
    "        fraction_outlier = len(outliers) / len(deltabias)\n",
    "        fraction_outliers.append(fraction_outlier)\n",
    "    \n",
    "        #2 sigma\n",
    "        outliers2 = deltabias[np.abs(deltabias-mean_bias) > 2 * sigma]\n",
    "        fraction_outlier2 = len(outliers2) / len(deltabias)\n",
    "        outliers_2.append(fraction_outlier2)\n",
    "\n",
    "    points = np.delete(points, drop_bins)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(8, 8), sharex=True)\n",
    "    plt.subplots_adjust(hspace=0.1) \n",
    "\n",
    "    axes[1]\n",
    "    x_lim = (0, np.max(bins))\n",
    "\n",
    "    # Subplot 1: Mean Bias\n",
    "    axes[0].plot(points[:-1], meanz, 'bo-')\n",
    "    axes[0].axhline(0, color='black', linestyle='--')\n",
    "    axes[0].set_ylabel(r'$\\Delta z$', fontsize=20)\n",
    "    axes[0].set_xlim(x_lim)\n",
    "    #axes[0].set_ylim(-0.05, 0.05)\n",
    "    axes[0].tick_params(axis='both', labelsize=14)\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Subplot 2: Sigma 68\n",
    "    axes[1].plot(points[:-1], sigma68z, 'go-')\n",
    "    axes[1].set_ylabel(r'$\\sigma_{68}$', fontsize=20)\n",
    "    axes[1].set_xlim(x_lim)\n",
    "    axes[1].axhline(0.12, color='black', linestyle='--')\n",
    "    #axes[1].set_ylim(0,max(sigmaz)+0.01)\n",
    "    #axes[1].set_ylim(0, 0.03)\n",
    "    axes[1].set_xlim(x_lim)\n",
    "    axes[1].axhline(0.12, color='black', linestyle='--')\n",
    "    #axes[1].set_ylim(0,max(sigmaz)+0.01)\n",
    "    #axes[1].set_ylim(0, 0.03)\n",
    "    axes[1].set_yticks(np.arange(0, 0.25, 0.05))\n",
    "    axes[1].tick_params(axis='both', labelsize=14)\n",
    "    axes[1].grid(True)\n",
    "\n",
    "\n",
    "    # Subplot3: 2_outliers\n",
    "    axes[2].plot(points[:-1],outliers_2,'o-',color='darkorange')\n",
    "    #axes[2].set_xlabel(r'$Z_{phot}$', fontsize=20)\n",
    "    axes[2].set_ylabel('out$_{2σ}$', fontsize=20)\n",
    "    axes[2].set_xlim(x_lim)\n",
    "    #axes[2].set_ylim(0,0.12)\n",
    "    axes[2].axhline(0.1, color='black', linestyle='--')\n",
    "    axes[2].tick_params(axis='both', labelsize=14)\n",
    "    axes[2].grid(True)\n",
    "    #axes[2].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "    # Subplot 4: 3_outliers\n",
    "    axes[3].plot(points[:-1], fraction_outliers, 'ro-')\n",
    "    axes[3].set_xlabel(r'$Z_{spec}$', fontsize=20)\n",
    "    axes[3].set_ylabel('out$_{3σ}$', fontsize=20)\n",
    "    axes[3].set_xlim(x_lim)\n",
    "    #axes[3].set_ylim(0,0.12)\n",
    "    axes[3].axhline(0.1, color='black', linestyle='--')\n",
    "    axes[3].tick_params(axis='both', labelsize=14)\n",
    "    axes[3].grid(True)\n",
    "   \n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.xlim(0, 1.6)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097918b-80ea-4802-b259-fc334d1a2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(y_test, y_pred_rf, 1.6, initial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d12ac8-6f9e-4095-8b88-c794f1ba5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(y_test, y_pred_dt, 1.6, initial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f25bc-e2dc-4345-a7ea-9f2258dd7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(y_test, y_pred_mlp, 1.6, initial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dabd5a-8878-4246-93e8-67748fa2aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6), sharex=False)\n",
    "plt.subplots_adjust(hspace=0.)\n",
    "\n",
    "ax = axes[0]\n",
    "#ax.set_xlabel('z', fontsize = 17)\n",
    "ax.grid(color = 'gray', linestyle = '--', linewidth = 0.5) \n",
    "_ = ax.hist(y_test, bins=np.arange(0, 1.6, 0.025), density=False, color='C0', alpha=0.5)\n",
    "_ = ax.hist(y_pred_rf, bins=np.arange(0, 1.6, 0.025), density=False, color='C1', alpha=0.5)\n",
    "#ax.set_ylim(0, 160)\n",
    "ax.legend(['y_test', 'y_pred'])\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "#ax.set_xlabel('z', fontsize = 17)\n",
    "ax.grid(color = 'gray', linestyle = '--', linewidth = 0.5) \n",
    "_ = ax.hist(y_test, bins=np.arange(0, 1.6, 0.025), density=False, color='C0', alpha=0.5)\n",
    "_ = ax.hist(y_pred_dt, bins=np.arange(0, 1.6, 0.025), density=False, color='C1', alpha=0.5)\n",
    "#ax.set_ylim(0, 160)\n",
    "ax.legend(['y_test', 'y_pred'])\n",
    "    \n",
    "ax = axes[2]\n",
    "\n",
    "#ax.set_xlabel('z', fontsize = 17)\n",
    "ax.grid(color = 'gray', linestyle = '--', linewidth = 0.5) \n",
    "_ = ax.hist(y_test, bins=np.arange(0, 1.6, 0.025), density=False, color='C0', alpha=0.5)\n",
    "_ = ax.hist(y_pred_mlp, bins=np.arange(0, 1.6, 0.025), density=False, color='C1', alpha=0.5)\n",
    "#ax.set_ylim(0, 160)\n",
    "ax.legend(['y_test', 'y_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6fc59",
   "metadata": {},
   "source": [
    "# 4. Decision Tree Regression - With colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47d3cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first CSV file (assumes it contains the test data)\n",
    "data = pd.read_csv(\"./data/des_match_vvds_clean.csv\")\n",
    "\n",
    "# The 'z' columns is the target and the 'mag_*' and 'mag_err_' columns are the predictor\n",
    "N = len(data)\n",
    "X_colors = np.zeros((N, 4))\n",
    "\n",
    "X_colors[:, 0] = data['mag_auto_g_dered'] - data['mag_auto_r_dered']\n",
    "X_colors[:, 1] = data['mag_auto_r_dered'] - data['mag_auto_i_dered']\n",
    "X_colors[:, 2] = data['mag_auto_i_dered'] - data['mag_auto_z_dered']\n",
    "X_colors[:, 3] = data['mag_auto_r_dered'] - data['mag_auto_z_dered']\n",
    "z = data['z']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0afd7",
   "metadata": {},
   "source": [
    "**Ex: Split the data into training and test sets (e.g., 70/30 split)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd4a78",
   "metadata": {},
   "source": [
    "**Ex: Repeat the decision tree analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Decision Tree - Magnitudes\")\n",
    "plt.scatter(y_test, y_pred_dt, s=2, color=\"C1\")\n",
    "x = np.linspace(0, 1.6, 100)\n",
    "plt.plot(x, x, \"k--\", alpha=0.5)\n",
    "plt.ylim(0, 1.6)\n",
    "plt.xlim(0, 1.6)\n",
    "plt.xlabel(r\"$z_\\mathrm{true}$\")\n",
    "plt.ylabel(r\"$z_\\mathrm{phot}$\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Decision Tree - Colors\")\n",
    "plt.scatter(ztest, zpred_color, s=2, color=\"C1\")\n",
    "x = np.linspace(0, 1.6, 100)\n",
    "plt.plot(x, x, \"k--\", alpha=0.5)\n",
    "plt.ylim(0, 1.6)\n",
    "plt.xlim(0, 1.6)\n",
    "plt.xlabel(r\"$z_\\mathrm{true}$\")\n",
    "plt.ylabel(r\"$z_\\mathrm{phot}$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efex7_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
